---
title: Redis 高可用之 Sentinel 部署与原理

tags:
  - redis
  - sentinel

categories:
  - Redis

comments: true
date: 2016-01-12 13:00:00

---

上一篇文章中我们介绍的 redis 的主从复制机制，在此基础上，redis 提供了 sentinel 这一特性实现单机 redis 的高可用，并实现 auto-failover。

# 简介

Redis 的 Sentinel 本身是一个分布式系统，这样就避免了 Sentinel 成为一个单点。部署 Sentinel 要求最少启动 3 个 sentinel 实例，因为在 auto-failover 时，sentinle 内部会选举 leader，然后由 leader 实现 auto-failover。

为了保证 sentinel 的鲁棒性以及可靠性，最好将 sentinel 实例分别部署在不同的物理机或物理环境中，尽量避免多个 sentinel 因为一台设备宕机同时挂掉。Sentinel 之前需要通信会话来实现选举、检测宕机、同步状态等等，所以每个 sentinel 启动的端口一定要保证其他 sentinel 实例可访问。

Sentinel 启动必须指定配置文件，他的配置文件不单单是读取配置的作用，同时用作实时的状态存储。

每个 sentinel 实例可以理解为一个 运行在 sentinel 模式下的 redis 实例。每个 sentinel 实例会创建两个向 redis  master/slave 的链接，其中一个链接用于发送命令，另一个链接是利用 redis  pubsub 功能和其他 sentinel 实例同步信息的。

# 工作原理

## 通信
Sentinel 启动后会向其监控的 redis 实例(master or slave)建立两条连接：
-  一条命令连接：用来向 redis 实例发送 redis 命令，比如 info 命令，获取 实例信息
-  一个订阅连接：被监控的 redis 实例会创建一个 _sentinel_:hello 的频道，sentinel 会订阅并发布命令

默认情况下，sentinel 会每 10s 向监控的 redis 实例发送 info 命令，获取相关信息，并记录或更新对应 redis 实例的信息。

同样的，sentinel 也会每 2s 向监控的 redis 实例的 _sentinel_:hello 频道发送一条信息（PUBSUB），信息包括此 sentinel 自身的一些参数，如 ip、port、runid，以及被监控的这个 redis 实例的信息，如 ip、port、name。

同时，sentinel 也订阅（SUBSCRIBE）了监控的 redis 实例的_sentinel_:hello 频道，也就是说一个 sentinel 发送到监控的 redis 实例订阅频道的信息会同时被其他监控此 redis 实例的 sentinel 收到，包括他自己。所以，监控同一个 redis 实例的所有 sentinel 之间通过此频道互相发现，然后会记录或更新其他监控这个 redis 的 sentinel 的相关信息。

当 sentinel 发现一个新的 sentinel 后，会互相建立两条命令连接用以通信。

## 检测实例下线

连接建立完成后，sentinel 会不断向其他 sentinel 和所有监控的 redis 实例发生 PING （默认每 1s 一次），根据配置 `down-after-milliseconds`，当指定的时间内，如果其他 sentinel 或 redis 做出了无效恢复（包括未回复），则此 sentinel 认为对应实例 down（此时为主观下线）。

（注意，不同的 sentinel 此配置可能不同。）

当认定主观下线后，sentinel 会向其他 sentinel 确认，当有收到足够的确认回复后，此 sentinel 会认定此实例客观下线，准备开始实行故障转移。认定客观下线的确认值通过 `sentinel monitor ` 命令的 `quorum` 参数决定。

## 选举 leader
选举的策略大概可以概括为：
- 每个认定实例客观下线的 sentinel 都会想其他 sentinel 发送请求选择自己成为局部 leader. 发送命令为

```
# 当 runid 是* 时这个命令用以确认客观下线
# 而是 sentinel id 时，用以请求被选为 leader
SENTINEL is-master-down-by-addr <ip> <port> <current_epoch> <runid>
```

- 收到此请求的 sentinel 根据先到先得的原则，将接受到第一个请求成为 leader 的 sentinel 选择为 leader.
- 当一个 sentinel 收到大于半数的选票，比如 3 个 sentinel 的集群大于 2 个时，则成为 leader
- 若在一轮选举中，没有 leader 产生，则各个 sentinel 会一段时间后再次选举.
- sentinel 通过 configuration epoch 这个配置确保 sentinel 通信的有效性。这是值本质是一个计数器，每轮选举 +1 。每个 sentinel 在一轮选举中只能选择做出一次选择。而上文 `current_epoch` 这个参数就是用来确定当前选举的轮数，计算选票是，只有 current_epoch 必须相同，才能算作有效选举。

## Failover

选举成为 leader 的 sentinel 会执行此次 failover，具体过程如下：

- 从所有 slave 中选出一个 slave，并将其设定为 master，向其发送 `slaveof no one`
- 将对应的其他 slave 的复制目标改为新的 master，向其他 slave 发送新的 `slaveof master` 命令
- 将 down 的 master 改为从新 master 的 slave，这一步会先保存，当 down 的 master 从新上线时，会发送 `slaveof master` 命令

sentinel 选择新的 master 的一些原则：

- 过滤： 过滤 down 和 最近 5s 内没有回复的 slave，保证剩余 slave 最近可用。同时过滤掉与 down master 超过一定时间 ( down-after-millseconds ) 没有通信的 slave，保证剩余的 slave 中数据比较新
- 比较优先级：sentinel 会选择优先级最高的 slave
- 比较 offset ：当具有多个优先级最高的 slave 时，sentinel 会选择 offset 最大的，意味着其数据最新
- 比较 runid :   当优先级和 offset 都相同时，sentinel 选择 runid 最小的.

总之，sentinel 选择新 master 的原则是`最近可用` 且 `数据最新` 且 `优先级最高` 且 `活跃最久` ！

# 部署

首先按上文部署一套 Redis 主从，其中 master 运行在 “127.0.0.1：6379”，slave 部署在 “127.0.0.1：6380”，如下：

![redis replication](http://om2dgc3yh.bkt.clouddn.com/1.jpeg)

分别创建三个 Sentinel 实例的配置文件，并添加一下配置：

```
# port <sentinel-port>
port 26379  #注意这里三个配置文件的端口分别配置为 26379/26380/26381

# dir <working-directory>
dir "/tmp"

# sentinel monitor <master-name> <ip> <redis-port> <quorum>
sentinel monitor mymaster 127.0.0.1 6379 2

# sentinel down-after-milliseconds <master-name> <milliseconds>
sentinel down-after-milliseconds mymaster 60000

# sentinel failover-timeout <master-name> <milliseconds>
sentinel failover-timeout mymaster 180000

# sentinel parallel-syncs <master-name> <numslaves>
sentinel parallel-syncs mymaster 1
```

配置文件的第一行指明了 Sentinel 实例运行的端口， 第二行是工作目录，第三行是重点了，从第三个参数 `mymaster` 开始，四个参数分别对应：

*   mymaster，应用于被监控 redis 主从的 group 名称，可自取
*   127.0.0.1 和 6379 分别是待监控 redis  master 的地址
*   2 表示要至少有两个及以上 Sentinel 实例同意认定 redis master 实例不可达，才可以开始 failover，因为我们启动了一个 三个实例的 Sentinel 集群，所以设置这个值为 2 (3/2+1=2，超过半数)。

第四行的配置表示 Sentinel 实例尝试连接 redis master 超过 60s 后才认为 redis 宕掉。第五行配置设定了 Sentinel 执行一次 failover 的超时时间。配置好配置文件后启动三个 Sentinel 实例：

> redis-sentinel /usr/local/etc/redis-sentinel-1.conf
> redis-sentinel /usr/local/etc/redis-sentinel-2.conf
> redis-sentinel /usr/local/etc/redis-sentinel-3.conf

其后控制台输出日历如下：
![ setinel console](http://om2dgc3yh.bkt.clouddn.com/2.jpeg)

从日志可以看到三个 Sentinel 实例都成功启动，成功监控 redis master “6379” 和 slave “6380”，并都监听到了其他两个 Sentinel 实例，组成一个集群 。

这时分别观察 Sentinel 的配置文件可以发现，每个Sentinel 实例的配置文件最后都出现了相应的 “***# Generated by CONFIG REWRITE***”，分别记录了监听到的 redis slave 地址以及其他两个 Sentinel 实例的 ID。
![ setinel 配置文件新增部分](http://om2dgc3yh.bkt.clouddn.com/3.jpeg)

现在，我们 kill 掉 redis 的 master 进程，观察 `d51f......` Sentinel 实例的日志可以发现，大约 1min 后，此实例认为 redis master 宕机，试图进行 failover，首先它投票自己作为 Sentinel leader 实施此次 failover，其后收到另外两个 Sentinel 实例的 vote，其成为 leader，然后其选定 slave 6380 作为 master 替换掉 6379，并认定 slave 6379 为 slave 且处于宕机状态。
![auto-failover](http://om2dgc3yh.bkt.clouddn.com/4.jpeg)

观察其他两**非** Sentinel leader 实例的日志可以发现一条有趣的日志:
![failover delay](http://om2dgc3yh.bkt.clouddn.com/5.jpeg)

这表示在 6min 内不会 “start a failover”，正是上文第五行配置的超时时间的 2 倍。因为这个配置时间其实还要有很多意义：

1.  重启一次 failover 的最小时间是这个配置时间的 2倍，即这条日志的情况
2.  slave 复制从错误的 mastet 转向正确 master 的超时时间
3.  取消一次已经正在进行的 failover 的超时时间
4.  failover 进程等待所有 slaves 转向 new master 复制的超时时间

其实后 2-4 条反应了第 3 条配置超时时间 2 倍的原因~

完成 failover 后，Sentinel各实例对应的配置文件也发现的变化，其中 monitor 配置中的 master 地址变成了 “6380”，而 known-slave 配置从原来的 “6380” 变成的 “6379”。
![ slave 变更](http://om2dgc3yh.bkt.clouddn.com/6.jpeg)

# Client

当然 redis 变成 Sentinel 模式后，client 链接的方式也要变一下，不能直接使用之前直接连接向 redis server 的方式，转而使用连接到 sentinel 的方式，从 sentinel 获取 master 和 slave 的链接，以 redis-py 为例：

> In [6]: from redis import sentinel
> In [7]: from redis.sentinel import Sentinel
> In [8]: sentinel = Sentinel([('127.0.0.1', 26379),('127.0.0.1', 26380),('127.0.0.1',26381)], socket_timeout=1)
> In [9]: master = sentinel.master_for('mymaster', socket_timeout=0.1)
> In [10]: slave = sentinel.slave_for('mymaster', socket_timeout=0.1)

maste  和 slave 的用法和 redis client 的用法一致：
![ setinel log](http://om2dgc3yh.bkt.clouddn.com/7.jpeg)
