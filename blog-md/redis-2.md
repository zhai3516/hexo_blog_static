---
title: Redis 高可用之 Sentinel 简介与配置

tags:
  - redis
  - sentinel

categories:
  - Redis

comments: true
date: 2016-01-12 13:00:00

---

上一篇文章中我们介绍的 redis 的主从复制机制，在此基础上，redis 提供了 sentinel 这一特性实现单机 redis 的高可用，并实现 auto-failover。

# 简介

Redis 的 Sentinel 本身是一个分布式系统，这样就避免了 Sentinel 成为一个单点。部署 Sentinel 要求最少启动 3 个 sentinel 实例，因为在 auto-failover 时，sentinle 内部会选举 leader，然后由 leader 实现 auto-failover。

为了保证 sentinel 的鲁棒性以及可靠性，最好将 sentinel 实例分别部署在不同的物理机或物理环境中，尽量避免多个 sentinel 因为一台设备宕机同时挂掉。Sentinel 之前需要通信会话来实现选举、检测宕机、同步状态等等，所以每个 sentinel 启动的端口一定要保证其他 sentinel 实例可访问。

Sentinel 启动必须指定配置文件，他的配置文件不单单是读取配置的作用，同时用作实时的状态存储。

每个 sentinel 实例可以理解为一个 运行在 sentinel 模式下的 redis 实例。每个 sentinel 实例会创建两个向 redis  master/slave 的链接，其中一个链接用于发送命令，另一个链接是利用 redis  pubsub 功能和其他 sentinel 实例同步信息的。

# 部署

首先按上文部署一套 Redis 主从，其中 master 运行在 “127.0.0.1：6379”，slave 部署在 “127.0.0.1：6380”，如下：

![redis replication](http://om2dgc3yh.bkt.clouddn.com/1.jpeg)

分别创建三个 Sentinel 实例的配置文件，并添加一下配置：

```
# port <sentinel-port>
port 26379  #注意这里三个配置文件的端口分别配置为 26379/26380/26381

# dir <working-directory>
dir "/tmp"

# sentinel monitor <master-name> <ip> <redis-port> <quorum>
sentinel monitor mymaster 127.0.0.1 6379 2

# sentinel down-after-milliseconds <master-name> <milliseconds>
sentinel down-after-milliseconds mymaster 60000

# sentinel failover-timeout <master-name> <milliseconds>
sentinel failover-timeout mymaster 180000

# sentinel parallel-syncs <master-name> <numslaves>
sentinel parallel-syncs mymaster 1
```

配置文件的第一行指明了 Sentinel 实例运行的端口， 第二行是工作目录，第三行是重点了，从第三个参数 `mymaster` 开始，四个参数分别对应：

*   mymaster，应用于被监控 redis 主从的 group 名称，可自取
*   127.0.0.1 和 6379 分别是待监控 redis  master 的地址
*   2 表示要至少有两个及以上 Sentinel 实例同意认定 redis master 实例不可达，才可以开始 failover，因为我们启动了一个 三个实例的 Sentinel 集群，所以设置这个值为 2 (3/2+1=2，超过半数)。

第四行的配置表示 Sentinel 实例尝试连接 redis master 超过 60s 后才认为 redis 宕掉。第五行配置设定了 Sentinel 执行一次 failover 的超时时间。配置好配置文件后启动三个 Sentinel 实例：

> redis-sentinel /usr/local/etc/redis-sentinel-1.conf
> redis-sentinel /usr/local/etc/redis-sentinel-2.conf
> redis-sentinel /usr/local/etc/redis-sentinel-3.conf

其后控制台输出日历如下：
![ setinel console](http://om2dgc3yh.bkt.clouddn.com/2.jpeg)

从日志可以看到三个 Sentinel 实例都成功启动，成功监控 redis master “6379” 和 slave “6380”，并都监听到了其他两个 Sentinel 实例，组成一个集群 。

这时分别观察 Sentinel 的配置文件可以发现，每个Sentinel 实例的配置文件最后都出现了相应的 “***# Generated by CONFIG REWRITE***”，分别记录了监听到的 redis slave 地址以及其他两个 Sentinel 实例的 ID。
![ setinel 配置文件新增部分](http://om2dgc3yh.bkt.clouddn.com/3.jpeg)

现在，我们 kill 掉 redis 的 master 进程，观察 `d51f......` Sentinel 实例的日志可以发现，大约 1min 后，此实例认为 redis master 宕机，试图进行 failover，首先它投票自己作为 Sentinel leader 实施此次 failover，其后收到另外两个 Sentinel 实例的 vote，其成为 leader，然后其选定 slave 6380 作为 master 替换掉 6379，并认定 slave 6379 为 slave 且处于宕机状态。
![auto-failover](http://om2dgc3yh.bkt.clouddn.com/4.jpeg)

观察其他两**非** Sentinel leader 实例的日志可以发现一条有趣的日志:
![failover delay](http://om2dgc3yh.bkt.clouddn.com/5.jpeg)

这表示在 6min 内不会 “start a failover”，正是上文第五行配置的超时时间的 2 倍。因为这个配置时间其实还要有很多意义：

1.  重启一次 failover 的最小时间是这个配置时间的 2倍，即这条日志的情况
2.  slave 复制从错误的 mastet 转向正确 master 的超时时间
3.  取消一次已经正在进行的 failover 的超时时间
4.  failover 进程等待所有 slaves 转向 new master 复制的超时时间

其实后 2-4 条反应了第 3 条配置超时时间 2 倍的原因~

完成 failover 后，Sentinel各实例对应的配置文件也发现的变化，其中 monitor 配置中的 master 地址变成了 “6380”，而 known-slave 配置从原来的 “6380” 变成的 “6379”。
![ slave 变更](http://om2dgc3yh.bkt.clouddn.com/6.jpeg)

# Client

当然 redis 变成 Sentinel 模式后，client 链接的方式也要变一下，不能直接使用之前直接连接向 redis server 的方式，转而使用连接到 sentinel 的方式，从 sentinel 获取 master 和 slave 的链接，以 redis-py 为例：

> In [6]: from redis import sentinel
> In [7]: from redis.sentinel import Sentinel
> In [8]: sentinel = Sentinel([('127.0.0.1', 26379),('127.0.0.1', 26380),('127.0.0.1',26381)], socket_timeout=1)
> In [9]: master = sentinel.master_for('mymaster', socket_timeout=0.1)
> In [10]: slave = sentinel.slave_for('mymaster', socket_timeout=0.1)

maste  和 slave 的用法和 redis client 的用法一致：
![ setinel log](http://om2dgc3yh.bkt.clouddn.com/7.jpeg)
